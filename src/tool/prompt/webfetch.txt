Fetch content from a URL and return it as processed text.

IMPORTANT: WebFetch WILL FAIL for authenticated or private URLs. Before using this tool, check if the URL points to an authenticated service (e.g. Google Docs, Confluence, Jira, private GitHub repos). For GitHub URLs, prefer using `bash` with `gh` CLI instead (e.g. gh pr view, gh issue view, gh api).

Use cases:
- Retrieve documentation or API references from public URLs
- Read web pages for research
- Fetch remote configuration or data

Parameters:
- url (required): URL to fetch (must be a valid HTTP/HTTPS URL)
- prompt (optional): instruction for how to process/extract information from the content

Notes:
- HTML is automatically converted to Markdown for readability
- Large content (over 50KB) is truncated
- Only use for public, unauthenticated URLs
- HTTP URLs are automatically upgraded to HTTPS
- Includes a 30-second timeout
- Includes a self-cleaning 15-minute cache for faster repeat access
- Do NOT use for local file reading — use `read` instead
- Do NOT generate or guess URLs. Only use URLs provided by the user or found in project files.

Examples:
<good-example>
# Fetch public documentation
webfetch url="https://docs.example.com/api" prompt="Extract all REST endpoints and their parameters"
</good-example>
<bad-example>
# WRONG: authenticated URL — will fail
webfetch url="https://docs.google.com/document/d/abc123/edit"
# Instead: ask the user to paste the content or use an MCP tool with auth
</bad-example>
<bad-example>
# WRONG: GitHub URL — use gh CLI instead
webfetch url="https://github.com/org/repo/pull/42"
# Instead: bash "gh pr view 42"
</bad-example>